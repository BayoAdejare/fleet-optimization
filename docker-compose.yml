version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_4LW_COMMANDS_WHITELIST: "srvr,ruok,stat,mntr,cons"
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD", "bash", "-c", "echo ruok | nc localhost 2181 | grep imok"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
    networks:
      - fleet-network

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    ports:
      - "9092:9092"
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 6
      start_period: 75s
    volumes:
      - ./scripts/kafka_topics.sh:/usr/local/bin/kafka_topics.sh
    networks:
      - fleet-network

  postgres:
    image: timescale/timescaledb:2.11.2-pg14
    environment:
      POSTGRES_DB: fleet
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d fleet"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./infrastructure/docker/initdb:/docker-entrypoint-initdb.d
    networks:
      - fleet-network

  minio:
    image: minio/minio:RELEASE.2023-09-04T19-57-37Z
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - fleet-network

  mlflow:
    build: 
      context: .
      dockerfile: infrastructure/docker/mlflow/Dockerfile
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
    ports:
      - "5000:5000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/api/2.0/mlflow/experiments/search?max_results=1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 120s
    command: >
      mlflow server
      --backend-store-uri postgresql://postgres:postgres@postgres:5432/fleet
      --default-artifact-root s3://mlflow-artifacts/
      --host 0.0.0.0
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    networks:
      - fleet-network

  app:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - POSTGRES_URL=postgresql://postgres:postgres@postgres:5432/fleet
      - MINIO_URL=http://minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin 
      - AWS_REGION=us-east-1
      - MLFLOW_S3_USE_PATH_STYLE=true
      - MLFLOW_BUCKET_NAME=mlflow-artifacts
    volumes:
      - .:/app
      - /app/.venv
      - ./mlflow-artifacts:/app/mlflow-artifacts
    ports:
      - "8000:8000"
      - "8050:8050"
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      mlflow:
        condition: service_healthy
    networks:
      - fleet-network
    command: >
      bash -c "python scripts/wait_for_services.py &&
               python scripts/cleanup_minio.py &&
               python scripts/init_minio.py &&
               python scripts/seed_demo_data.py &&
               python scripts/ensure_model_exists.py &&
               faust -A src.data.stream_processor worker &&
               python src/api/app.py"
               
  grafana:
    image: grafana/grafana:10.0.3
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    ports:
      - "3000:3000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    volumes:
      - grafana_data:/var/lib/grafana
      - ./infrastructure/docker/grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - postgres
    networks:
      - fleet-network

volumes:
  postgres_data:
    # Simple named volume - compatible with both Docker and Podman
  minio_data:
    # Simple named volume - removed problematic driver_opts
    # For development, this uses default storage
    # For production, consider external volume with proper storage management
  grafana_data:
    # Simple named volume - compatible with both

networks:
  fleet-network:
    driver: bridge
